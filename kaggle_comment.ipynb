{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2798066,"sourceType":"datasetVersion","datasetId":1709138}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-09T13:28:19.520723Z","iopub.execute_input":"2024-01-09T13:28:19.521098Z","iopub.status.idle":"2024-01-09T13:28:19.528036Z","shell.execute_reply.started":"2024-01-09T13:28:19.521066Z","shell.execute_reply":"2024-01-09T13:28:19.527112Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"/kaggle/input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv\n/kaggle/input/jigsaw-toxic-comment-classification-challenge/test_labels.csv\n/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv\n/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.offline as pyo\nimport plotly.graph_objects as go\nimport re\nfrom wordcloud import WordCloud, STOPWORDS\n\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\n\nimport os\nimport zipfile\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-01-09T13:28:34.499970Z","iopub.execute_input":"2024-01-09T13:28:34.500369Z","iopub.status.idle":"2024-01-09T13:28:34.507529Z","shell.execute_reply.started":"2024-01-09T13:28:34.500340Z","shell.execute_reply":"2024-01-09T13:28:34.506520Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"kaggle_input_path = '/kaggle/input/jigsaw-toxic-comment-classification-challenge'\n\nfiles_in_directory = os.listdir(kaggle_input_path)\nprint(files_in_directory)\n\n\nfor file_name in files_in_directory:\n    # Skip non-zip files if any\n    if not file_name.endswith('.zip'):\n        continue\n\n    # Path to the zip file\n    zip_file_path = os.path.join(kaggle_input_path, file_name)\n\n    # Directory where you want to extract the contents\n    output_dir = '/kaggle/working/'\n\n    # Unzip the file\n    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n        zip_ref.extractall(output_dir)\n        # Print complete paths of the extracted files\n        extracted_files = zip_ref.namelist()\n        for extracted_file in extracted_files:\n            complete_path = os.path.join(output_dir, extracted_file)\n            print(\"Extracted:\", complete_path)        ","metadata":{"execution":{"iopub.status.busy":"2024-01-09T13:29:39.214330Z","iopub.execute_input":"2024-01-09T13:29:39.215034Z","iopub.status.idle":"2024-01-09T13:29:39.223825Z","shell.execute_reply.started":"2024-01-09T13:29:39.214999Z","shell.execute_reply":"2024-01-09T13:29:39.222843Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"['sample_submission.csv', 'test_labels.csv', 'train.csv', 'test.csv']\n","output_type":"stream"}]},{"cell_type":"code","source":"train = pd.read_csv(kaggle_input_path+\"/train.csv\")\ntrain.sample(5)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T13:29:39.852076Z","iopub.execute_input":"2024-01-09T13:29:39.852764Z","iopub.status.idle":"2024-01-09T13:29:40.793430Z","shell.execute_reply.started":"2024-01-09T13:29:39.852731Z","shell.execute_reply":"2024-01-09T13:29:40.792472Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"                      id                                       comment_text  \\\n148578  4ef2da8ee996b7d9  See man you really dont get it, why do you thi...   \n145885  23bf8eb990db3a85  \"\\n\\n RADIUS Question \\n\\nI'm taking a Securit...   \n7967    153990b9dd2f7ef4  \"\\n\\nIslamic terrorism\\nHi again Sir Nicho. I ...   \n84563   e23a26e0ca5ba8db  Rose in Richmond, Indiana\\nYour last edit was ...   \n50431   86d9802508ce35f2  Adil Asrar \\nHello and welcome to Wikipedia.  ...   \n\n        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n148578      0             0        0       0       0              0  \n145885      0             0        0       0       0              0  \n7967        0             0        0       0       0              0  \n84563       0             0        0       0       0              0  \n50431       0             0        0       0       0              0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>comment_text</th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>148578</th>\n      <td>4ef2da8ee996b7d9</td>\n      <td>See man you really dont get it, why do you thi...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>145885</th>\n      <td>23bf8eb990db3a85</td>\n      <td>\"\\n\\n RADIUS Question \\n\\nI'm taking a Securit...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7967</th>\n      <td>153990b9dd2f7ef4</td>\n      <td>\"\\n\\nIslamic terrorism\\nHi again Sir Nicho. I ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>84563</th>\n      <td>e23a26e0ca5ba8db</td>\n      <td>Rose in Richmond, Indiana\\nYour last edit was ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>50431</th>\n      <td>86d9802508ce35f2</td>\n      <td>Adil Asrar \\nHello and welcome to Wikipedia.  ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train.columns","metadata":{"execution":{"iopub.status.busy":"2024-01-09T13:29:40.794831Z","iopub.execute_input":"2024-01-09T13:29:40.795092Z","iopub.status.idle":"2024-01-09T13:29:40.801543Z","shell.execute_reply.started":"2024-01-09T13:29:40.795070Z","shell.execute_reply":"2024-01-09T13:29:40.800588Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"Index(['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',\n       'insult', 'identity_hate'],\n      dtype='object')"},"metadata":{}}]},{"cell_type":"code","source":"def clean_text(text):\n    # Remove HTML tags\n    text = re.sub(r'<[^>]+>', '', text)\n\n    # Remove web links\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n\n    # Remove special characters, punctuation marks, and numbers\n    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n\n    # Insert spaces between certain patterns (e.g., \"ie\", \"eg\")\n    text = re.sub(r'(\\s)([iI][eE]|[eE][gG])(\\s)', r' \\2 ', text)\n\n    # Remove extra white spaces\n    text = \" \".join(text.split())\n\n    return text.lower()\n\n# Example usage with the provided text\ntexts = [\n    \"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of 'types of accidents'  -I think the references may need tidying so that they are all in the exact same format ie date format etc I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\n\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport\"\n]\n\ncleaned_texts = [clean_text(text) for text in texts]\nprint(cleaned_texts)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-09T13:29:40.932925Z","iopub.execute_input":"2024-01-09T13:29:40.933338Z","iopub.status.idle":"2024-01-09T13:29:40.940739Z","shell.execute_reply.started":"2024-01-09T13:29:40.933310Z","shell.execute_reply":"2024-01-09T13:29:40.939673Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"['more i can t make any real suggestions on improvement i wondered if the section statistics should be later on or a subsection of types of accidents i think the references may need tidying so that they are all in the exact same format ie date format etc i can do that later on if no one else does first if you have any preferences for formatting style on references or want to do it yourself please let me know there appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up it s listed in the relevant form eg wikipedia good article nominations transport']\n","output_type":"stream"}]},{"cell_type":"code","source":"target_labels= [col for col in train.columns if train[col].dtypes == 'int64']\ntarget_labels","metadata":{"execution":{"iopub.status.busy":"2024-01-09T13:29:41.439982Z","iopub.execute_input":"2024-01-09T13:29:41.440685Z","iopub.status.idle":"2024-01-09T13:29:41.447213Z","shell.execute_reply.started":"2024-01-09T13:29:41.440653Z","shell.execute_reply":"2024-01-09T13:29:41.446209Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"},"metadata":{}}]},{"cell_type":"code","source":"# Function to generate word cloud\ndef generate_wordcloud(text,Title):\n    wordcloud = WordCloud(width=800, height=400,stopwords=set(STOPWORDS), background_color='black').generate(text)\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.title(Title)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-09T13:29:41.897999Z","iopub.execute_input":"2024-01-09T13:29:41.898824Z","iopub.status.idle":"2024-01-09T13:29:41.904331Z","shell.execute_reply.started":"2024-01-09T13:29:41.898791Z","shell.execute_reply":"2024-01-09T13:29:41.903329Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"train['Cleaned_Comments'] = train['comment_text'].apply(clean_text)\ntrain['Cleaned_Comments'].head()","metadata":{"execution":{"iopub.status.busy":"2024-01-09T13:29:42.397032Z","iopub.execute_input":"2024-01-09T13:29:42.397722Z","iopub.status.idle":"2024-01-09T13:29:52.944702Z","shell.execute_reply.started":"2024-01-09T13:29:42.397690Z","shell.execute_reply":"2024-01-09T13:29:52.943687Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"0    explanation why the edits made under my userna...\n1    d aww he matches this background colour i m se...\n2    hey man i m really not trying to edit war it s...\n3    more i can t make any real suggestions on impr...\n4    you sir are my hero any chance you remember wh...\nName: Cleaned_Comments, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"Toxic_comment_balanced_1 = train[train['toxic'] == 1].iloc[0:15000,:]\nToxic_comment_balanced_0 = train[train['toxic'] == 0].iloc[0:20000,:]","metadata":{"execution":{"iopub.status.busy":"2024-01-09T13:29:52.946325Z","iopub.execute_input":"2024-01-09T13:29:52.946594Z","iopub.status.idle":"2024-01-09T13:29:52.989580Z","shell.execute_reply.started":"2024-01-09T13:29:52.946571Z","shell.execute_reply":"2024-01-09T13:29:52.988505Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"Toxic_comment_balanced=pd.concat([Toxic_comment_balanced_1,Toxic_comment_balanced_0])","metadata":{"execution":{"iopub.status.busy":"2024-01-09T13:29:52.992997Z","iopub.execute_input":"2024-01-09T13:29:52.993546Z","iopub.status.idle":"2024-01-09T13:29:53.006521Z","shell.execute_reply.started":"2024-01-09T13:29:52.993508Z","shell.execute_reply":"2024-01-09T13:29:53.005391Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"Toxic_comment_balanced.shape","metadata":{"execution":{"iopub.status.busy":"2024-01-09T13:29:53.009282Z","iopub.execute_input":"2024-01-09T13:29:53.010389Z","iopub.status.idle":"2024-01-09T13:29:53.016528Z","shell.execute_reply.started":"2024-01-09T13:29:53.010346Z","shell.execute_reply":"2024-01-09T13:29:53.015608Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"(35000, 9)"},"metadata":{}}]},{"cell_type":"code","source":"Toxic_comment_balanced_shuffled = Toxic_comment_balanced.sample(frac=1, random_state=42)  # You can set a random_state for reproducibility\n\n# Reset the index of the shuffled DataFrame\nToxic_comment_balanced_shuffled = Toxic_comment_balanced_shuffled.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T13:29:53.017869Z","iopub.execute_input":"2024-01-09T13:29:53.018517Z","iopub.status.idle":"2024-01-09T13:29:53.059786Z","shell.execute_reply.started":"2024-01-09T13:29:53.018485Z","shell.execute_reply":"2024-01-09T13:29:53.058804Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"comments=Toxic_comment_balanced_shuffled['Cleaned_Comments'].to_list()\ncomments[:5]","metadata":{"execution":{"iopub.status.busy":"2024-01-09T13:29:53.061313Z","iopub.execute_input":"2024-01-09T13:29:53.062134Z","iopub.status.idle":"2024-01-09T13:29:53.070560Z","shell.execute_reply.started":"2024-01-09T13:29:53.062093Z","shell.execute_reply":"2024-01-09T13:29:53.069664Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"['other naturally occurring opioid antagonists from wikipedia the compound akuammine has also been shown in vitro to act as an opioid antagonist',\n 'women bus drivers thoughts i don t like them what about you do you think it s right they should drive buses or do you think they should stick to washing up',\n 'stop changon it or i am twist off youre littal pee wees fuckan morons',\n 'you re a smug disruptive asshole i can t believe you re monitoring my talk and userpage so closely as to be replying minutes after i do',\n 'still not blocked hey you american asshole i am stil not blocked please do something and block me and all my socks']"},"metadata":{}}]},{"cell_type":"code","source":"# Split data into training, testing sets & validation sets \nTrain_texts, Test_texts, Train_labels, Test_labels = train_test_split(\n    comments, Toxic_comment_balanced_shuffled[target_labels].values, test_size=0.3, random_state=50)\n\n#validation set\ntest_texts, val_texts, test_labels, val_labels = train_test_split(\n    Test_texts, Test_labels, test_size=0.32, random_state=23)\n\nprint('Training Dataset -->',len(Train_texts), Train_labels.shape)\nprint('Testing Dataset -->',len(test_texts), test_labels.shape)\nprint('validation Dataset -->',len(val_texts), val_labels.shape)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T13:29:53.072000Z","iopub.execute_input":"2024-01-09T13:29:53.072433Z","iopub.status.idle":"2024-01-09T13:29:53.102107Z","shell.execute_reply.started":"2024-01-09T13:29:53.072398Z","shell.execute_reply":"2024-01-09T13:29:53.101012Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"Training Dataset --> 24500 (24500, 6)\nTesting Dataset --> 7140 (7140, 6)\nvalidation Dataset --> 3360 (3360, 6)\n","output_type":"stream"}]},{"cell_type":"code","source":"def tokenize_and_encode(tokenizer, comments, labels, max_length=128):\n    # Initialize empty lists to store tokenized inputs and attention masks\n    input_ids = []\n    attention_masks = []\n\n    # Iterate through each comment in the 'comments' list\n    for comment in comments:\n        # Tokenize and encode the comment using the BERT tokenizer\n        encoded_dict = tokenizer.encode_plus(\n            comment,\n            add_special_tokens=True,\n            max_length=max_length,\n            pad_to_max_length=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n\n        # Append the tokenized input and attention mask to their respective lists\n        input_ids.append(encoded_dict['input_ids'])\n        attention_masks.append(encoded_dict['attention_mask'])\n\n    # Convert the lists of tokenized inputs and attention masks to PyTorch tensors\n    input_ids = torch.cat(input_ids, dim=0)\n    attention_masks = torch.cat(attention_masks, dim=0)\n\n    # Convert the labels to a PyTorch tensor with the data type float32\n    labels = torch.tensor(labels, dtype=torch.float32)\n\n    # Return the tokenized inputs, attention masks, and labels as PyTorch tensors\n    return input_ids, attention_masks, labels\n        ","metadata":{"execution":{"iopub.status.busy":"2024-01-09T13:29:53.103383Z","iopub.execute_input":"2024-01-09T13:29:53.103723Z","iopub.status.idle":"2024-01-09T13:29:53.111251Z","shell.execute_reply.started":"2024-01-09T13:29:53.103697Z","shell.execute_reply":"2024-01-09T13:29:53.110202Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"# Token Initialization\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n\n# Model Initialization\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=6)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T13:29:53.112534Z","iopub.execute_input":"2024-01-09T13:29:53.112886Z","iopub.status.idle":"2024-01-09T13:29:53.800191Z","shell.execute_reply.started":"2024-01-09T13:29:53.112859Z","shell.execute_reply":"2024-01-09T13:29:53.799310Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-01-09T13:29:53.804744Z","iopub.execute_input":"2024-01-09T13:29:53.805097Z","iopub.status.idle":"2024-01-09T13:29:53.813023Z","shell.execute_reply.started":"2024-01-09T13:29:53.805069Z","shell.execute_reply":"2024-01-09T13:29:53.812048Z"},"trusted":true},"execution_count":52,"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=6, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Move model to GPU if available\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(device)\n\nmodel =model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T13:29:53.814273Z","iopub.execute_input":"2024-01-09T13:29:53.814638Z","iopub.status.idle":"2024-01-09T13:29:53.938879Z","shell.execute_reply.started":"2024-01-09T13:29:53.814606Z","shell.execute_reply":"2024-01-09T13:29:53.937888Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"# Tokenize and Encode the comments and labels for the training set\ninput_ids, attention_masks, labels = tokenize_and_encode(\n    tokenizer, \n    Train_texts, \n    Train_labels\n)\n\n# Step 4: Tokenize and Encode the comments and labels for the test set\ntest_input_ids, test_attention_masks, test_labels = tokenize_and_encode(\n    tokenizer,\n    test_texts,\n    test_labels\n)\n\n# Tokenize and Encode the comments and labels for the validation set\nval_input_ids, val_attention_masks, val_labels = tokenize_and_encode(\n    tokenizer,\n    val_texts,\n    val_labels\n)\n\nprint('Training Comments :',len(Train_texts))\nprint('Input Ids         :',input_ids.shape)\nprint('Attention Mask    :',attention_masks.shape)\nprint('Labels            :',labels.shape)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T13:29:53.940197Z","iopub.execute_input":"2024-01-09T13:29:53.940492Z","iopub.status.idle":"2024-01-09T13:31:20.838009Z","shell.execute_reply.started":"2024-01-09T13:29:53.940465Z","shell.execute_reply":"2024-01-09T13:31:20.836569Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"Training Comments : 24500\nInput Ids         : torch.Size([24500, 128])\nAttention Mask    : torch.Size([24500, 128])\nLabels            : torch.Size([24500, 6])\n","output_type":"stream"}]},{"cell_type":"code","source":"k = 523\nprint('Training Comments -->>',Train_texts[k])\nprint('\\nInput Ids -->>\\n',input_ids[k])\nprint('\\nDecoded Ids -->>\\n',tokenizer.decode(input_ids[k]))\nprint('\\nAttention Mask -->>\\n',attention_masks[k])\nprint('\\nLabels -->>',labels[k])","metadata":{"execution":{"iopub.status.busy":"2024-01-09T13:31:20.839662Z","iopub.execute_input":"2024-01-09T13:31:20.840036Z","iopub.status.idle":"2024-01-09T13:31:20.849480Z","shell.execute_reply.started":"2024-01-09T13:31:20.839992Z","shell.execute_reply":"2024-01-09T13:31:20.848555Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"Training Comments -->> please do not create articles on your talk page it creates a massive problem with your talk page history and the article s history and creates incorrect links because of the submission templates you have a sub page already made\n\nInput Ids -->>\n tensor([  101,  3531,  2079,  2025,  3443,  4790,  2006,  2115,  2831,  3931,\n         2009,  9005,  1037,  5294,  3291,  2007,  2115,  2831,  3931,  2381,\n         1998,  1996,  3720,  1055,  2381,  1998,  9005, 16542,  6971,  2138,\n         1997,  1996, 12339, 23561,  2015,  2017,  2031,  1037,  4942,  3931,\n         2525,  2081,   102,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0])\n\nDecoded Ids -->>\n [CLS] please do not create articles on your talk page it creates a massive problem with your talk page history and the article s history and creates incorrect links because of the submission templates you have a sub page already made [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n\nAttention Mask -->>\n tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0])\n\nLabels -->> tensor([0., 0., 0., 0., 0., 0.])\n","output_type":"stream"}]},{"cell_type":"code","source":"# Creating DataLoader for the balanced dataset\nbatch_size = 32\ntrain_dataset = TensorDataset(input_ids, attention_masks, labels)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n#test\ntest_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n#val\nval_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T13:31:20.850644Z","iopub.execute_input":"2024-01-09T13:31:20.850981Z","iopub.status.idle":"2024-01-09T13:31:20.862758Z","shell.execute_reply.started":"2024-01-09T13:31:20.850950Z","shell.execute_reply":"2024-01-09T13:31:20.861986Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"print('Batch Size :',train_loader.batch_size)\nBatch =next(iter(train_loader))\nprint('Each Input ids shape :',Batch[0].shape)\nprint('Input ids :\\n',Batch[0][0])\nprint('Corresponding Decoded text:\\n',tokenizer.decode(Batch[0][0]))\nprint('Corresponding Attention Mask :\\n',Batch[1][0])\nprint('Corresponding Label:',Batch[2][0])","metadata":{"execution":{"iopub.status.busy":"2024-01-09T13:31:20.863875Z","iopub.execute_input":"2024-01-09T13:31:20.864159Z","iopub.status.idle":"2024-01-09T13:31:20.875984Z","shell.execute_reply.started":"2024-01-09T13:31:20.864120Z","shell.execute_reply":"2024-01-09T13:31:20.875044Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"Batch Size : 32\nEach Input ids shape : torch.Size([32, 128])\nInput ids :\n tensor([  101,  2009,  2987,  1056,  3043,  2129,  2116,  1997,  2068,  2203,\n         1999, 21318,  2015,  2672,  2009,  1055,  2025,  2130, 10333,  1045,\n         2123,  1056,  2113,  3198,  6316,  1055,  3611,  2021, 21766,  2094,\n         4371,  3726, 10841,  2483,  2003,  1037,  2171,  2009,  4152,  4978,\n         1998,  2045,  2024, 15665,  1997,  2111,  2041,  2045,  2007,  2008,\n         2171,  3398, 21766,  2094,  4371,  7903,  4173,  2003,  2036,  1037,\n         2171,  1998,  2009,  1055,  2055,  2062,  2691,  2084, 21766,  2094,\n         4371,  3726, 10841,  2483,  2021,  6316, 21766,  2094,  4371,  7903,\n         4173,  1998, 10852, 21766,  2094,  4371,  7903,  4173,  2131,  5717,\n         4978,  2008,  1055,  1996,  2391,  6316, 21766,  2094,  4371,  3726,\n        25393,  4152,  2718,  1998, 10852, 21766,  2094,  4371,  3726, 25393,\n         4152,  5717,  6316, 21766,  2094,  4371,  3726, 10841,  2483,  1998,\n        10852, 21766,  2094,  4371,  3726, 10841,  2483,   102])\nCorresponding Decoded text:\n [CLS] it doesn t matter how many of them end in uis maybe it s not even lithuanian i don t know ask phil s dad but rudzevecuis is a name it gets hits and there are loads of people out there with that name yeah rudzevicius is also a name and it s about more common than rudzevecuis but phil rudzevicius and phillip rudzevicius get zero hits that s the point phil rudzevecius gets hit and phillip rudzevecius gets zero phil rudzevecuis and phillip rudzevecuis [SEP]\nCorresponding Attention Mask :\n tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1])\nCorresponding Label: tensor([0., 0., 0., 0., 0., 0.])\n","output_type":"stream"}]},{"cell_type":"code","source":"# Optimizer setup\noptimizer = AdamW(model.parameters(), lr=0.00002)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T13:31:20.877170Z","iopub.execute_input":"2024-01-09T13:31:20.877700Z","iopub.status.idle":"2024-01-09T13:31:20.885729Z","shell.execute_reply.started":"2024-01-09T13:31:20.877675Z","shell.execute_reply":"2024-01-09T13:31:20.884946Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, optimizer, device, num_epochs):\n    # Loop through the specified number of epochs\n    for epoch in range(num_epochs):\n        # Set the model to training mode\n        model.train() \n        # Initialize total loss for the current epoch\n        total_loss = 0  \n\n        # Loop through the batches in the training data\n        for batch in train_loader:\n            input_ids, attention_mask, labels = [t.to(device) for t in batch]\n\n            optimizer.zero_grad()\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            total_loss += loss.item()\n\n            loss.backward()\n            optimizer.step()\n\n        model.eval()  # Set the model to evaluation mode\n        val_loss = 0\n\n        # Disable gradient computation during validation\n        with torch.no_grad():\n            for batch in val_loader:\n                input_ids, attention_mask, labels = [t.to(device) for t in batch]\n\n                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n                loss = outputs.loss\n                val_loss += loss.item()\n\n        # Print the average loss for the current epoch\n        print(f'Epoch {epoch+1}, Training Loss: {total_loss/len(train_loader)}, Validation Loss: {val_loss/len(val_loader)}')\n\n# Assuming you have 'train_loader' and 'val_loader' defined elsewhere\n# Call the function to train the model\ntrain_model(model, train_loader, val_loader, optimizer, device, num_epochs=2)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T13:31:20.886717Z","iopub.execute_input":"2024-01-09T13:31:20.887272Z","iopub.status.idle":"2024-01-09T13:40:46.786357Z","shell.execute_reply.started":"2024-01-09T13:31:20.887247Z","shell.execute_reply":"2024-01-09T13:40:46.785262Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"Epoch 1, Training Loss: 0.1754996707473271, Validation Loss: 0.1381606556829952\nEpoch 2, Training Loss: 0.11272986035784294, Validation Loss: 0.1300277543209848\n","output_type":"stream"}]},{"cell_type":"code","source":"#Evaluate the Model\ndef evaluate_model(model, test_loader, device):\n    model.eval()  # Set the model to evaluation mode\n\n    true_labels = []\n    predicted_probs = []\n\n    with torch.no_grad():\n        for batch in test_loader:\n            input_ids, attention_mask, labels = [t.to(device) for t in batch]\n\n            # Get model's predictions\n            outputs = model(input_ids, attention_mask=attention_mask)\n            predicted_probs_batch = torch.sigmoid(outputs.logits)  # Use sigmoid for multilabel classification\n            predicted_probs.append(predicted_probs_batch.cpu().numpy())\n\n            true_labels_batch = labels.cpu().numpy()\n            true_labels.append(true_labels_batch)\n\n    # Combine predictions and labels for evaluation\n    true_labels = np.concatenate(true_labels, axis=0)\n    predicted_probs = np.concatenate(predicted_probs, axis=0)\n    predicted_labels = (predicted_probs > 0.5).astype(int)  # Apply threshold for binary classification\n\n    # Calculate evaluation metrics\n    accuracy = accuracy_score(true_labels, predicted_labels)\n    precision = precision_score(true_labels, predicted_labels, average='micro')\n    recall = recall_score(true_labels, predicted_labels, average='micro')\n\n    # Print the evaluation metrics\n    print(f'Accuracy: {accuracy:.4f}')\n    print(f'Precision: {precision:.4f}')\n    print(f'Recall: {recall:.4f}')\n\n# Call the function to evaluate the model on the test data\nevaluate_model(model, test_loader, device)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T13:40:46.787711Z","iopub.execute_input":"2024-01-09T13:40:46.788002Z","iopub.status.idle":"2024-01-09T13:41:11.491192Z","shell.execute_reply.started":"2024-01-09T13:40:46.787978Z","shell.execute_reply":"2024-01-09T13:41:11.490207Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"Accuracy: 0.7430\nPrecision: 0.8013\nRecall: 0.8869\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the tokenizer and model in the same directory\noutput_dir = \"/kaggle/working/Saved_model\"\nmodel.save_pretrained(output_dir)  # Save model's state dictionary and configuration\ntokenizer.save_pretrained(output_dir)  # Save tokenizer's configuration and vocabulary","metadata":{"execution":{"iopub.status.busy":"2024-01-09T13:41:11.492616Z","iopub.execute_input":"2024-01-09T13:41:11.493003Z","iopub.status.idle":"2024-01-09T13:41:12.718069Z","shell.execute_reply.started":"2024-01-09T13:41:11.492968Z","shell.execute_reply":"2024-01-09T13:41:12.717178Z"},"trusted":true},"execution_count":61,"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/Saved_model/tokenizer_config.json',\n '/kaggle/working/Saved_model/special_tokens_map.json',\n '/kaggle/working/Saved_model/vocab.txt',\n '/kaggle/working/Saved_model/added_tokens.json')"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}